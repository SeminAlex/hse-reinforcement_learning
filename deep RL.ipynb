{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#dependencies\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "\n",
    "The Gamma parameter has a range of 0 to 1 (0 <= Gamma > 1).  If Gamma is closer to zero, the agent will tend to consider only immediate rewards.  If Gamma is closer to one, the agent will consider future rewards with greater weight, willing to delay the reward.\n",
    "\n",
    "epsilon parameter determines with which probability our agent takes a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model\n",
    "first model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, envName=('CartPole-v0'), gamma=None, epsilon=None, e_decay=0.995, learning_rate=0.001):\n",
    "        self.envName = envName\n",
    "        self.env = gym.make(envName)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.e_decay = e_decay\n",
    "        self.e_min = 0.02  # minimal exploraton rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        builds a neural network\n",
    "        :return: model - neural network object\n",
    "        '''\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.env.observation_space.shape[0], activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        add <s,a,r,s'> vector to memory\n",
    "        :param state: agent state\n",
    "        :param action: action to perform\n",
    "        :param reward: reward given after performed action\n",
    "        :param next_state: next state\n",
    "        :param done: bool parameter which determines whrther current state terminal or not\n",
    "        :return: \n",
    "        '''\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        defins gow agent will actin a given state\n",
    "        :param state: state of our agent\n",
    "        :return: action to be perfomed\n",
    "        '''\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.env.action_space.n)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        '''\n",
    "        train network by sampling elements from memory\n",
    "        :param batch_size: sample size of agent memory\n",
    "        :return: \n",
    "        '''\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        X = np.zeros((batch_size, self.env.observation_space.shape[0]))\n",
    "        Y = np.zeros((batch_size, self.env.action_space.n))\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "            # print \"i\", i, \" predict\", self.model.predict(state)\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            X[i], Y[i] = state, target\n",
    "        self.model.fit(X, Y, batch_size=batch_size, nb_epoch=1, verbose=0)\n",
    "        if self.epsilon > self.e_min:\n",
    "            self.epsilon *= self.e_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        '''\n",
    "        load weights from a file\n",
    "        :param name: filename\n",
    "        :return: \n",
    "        '''\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        '''\n",
    "        seves weights to a file\n",
    "        :param name: filename\n",
    "        :return: \n",
    "        '''\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-17 13:20:00,571] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  718  score:  200.0\n",
      "episode  719  score:  200.0\n",
      "episode  722  score:  200.0\n",
      "episode  723  score:  200.0\n",
      "episode  725  score:  200.0\n",
      "episode  972  score:  200.0\n",
      "episode  977  score:  200.0\n",
      "episode  994  score:  200.0\n",
      "episode  1024  score:  200.0\n",
      "episode  1057  score:  197.0\n",
      "episode  1085  score:  200.0\n",
      "episode  1086  score:  200.0\n",
      "episode  1087  score:  200.0\n",
      "episode  1088  score:  200.0\n",
      "episode  1089  score:  200.0\n",
      "episode  1090  score:  200.0\n",
      "episode  1093  score:  200.0\n",
      "episode  1094  score:  200.0\n",
      "episode  1095  score:  200.0\n",
      "episode  1146  score:  182.0\n",
      "episode  1147  score:  200.0\n",
      "episode  1152  score:  195.0\n",
      "episode  1156  score:  200.0\n",
      "episode  1163  score:  200.0\n",
      "episode  1164  score:  200.0\n",
      "episode  1169  score:  200.0\n",
      "episode  1197  score:  200.0\n",
      "episode  1198  score:  200.0\n",
      "episode  1200  score:  200.0\n",
      "episode  1202  score:  200.0\n",
      "episode  1204  score:  196.0\n",
      "episode  1205  score:  200.0\n",
      "episode  1214  score:  200.0\n",
      "episode  1240  score:  200.0\n",
      "episode  1241  score:  200.0\n",
      "episode  1248  score:  200.0\n",
      "episode  1251  score:  200.0\n",
      "episode  1255  score:  200.0\n",
      "episode  1256  score:  200.0\n",
      "episode  1257  score:  200.0\n",
      "episode  1258  score:  200.0\n",
      "episode  1259  score:  197.0\n",
      "episode  1261  score:  200.0\n",
      "episode  1262  score:  200.0\n",
      "episode  1263  score:  200.0\n",
      "episode  1264  score:  200.0\n",
      "episode  1266  score:  200.0\n",
      "episode  1268  score:  187.0\n",
      "episode  1287  score:  200.0\n",
      "episode  1291  score:  183.0\n",
      "episode  1292  score:  200.0\n",
      "episode  1323  score:  200.0\n",
      "episode  1325  score:  200.0\n",
      "episode  1327  score:  200.0\n",
      "episode  1331  score:  183.0\n",
      "episode  1334  score:  200.0\n",
      "episode  1349  score:  200.0\n",
      "episode  1354  score:  200.0\n",
      "episode  1355  score:  200.0\n",
      "episode  1398  score:  200.0\n",
      "episode  1404  score:  200.0\n",
      "episode  1407  score:  200.0\n",
      "episode  1408  score:  200.0\n",
      "episode  1409  score:  200.0\n",
      "episode  1411  score:  200.0\n",
      "episode  1419  score:  200.0\n",
      "episode  1422  score:  200.0\n",
      "episode  1423  score:  200.0\n",
      "episode  1424  score:  200.0\n",
      "episode  1425  score:  200.0\n",
      "episode  1426  score:  200.0\n",
      "episode  1428  score:  200.0\n",
      "episode  1430  score:  200.0\n",
      "episode  1434  score:  200.0\n",
      "episode  1439  score:  200.0\n",
      "episode  1440  score:  200.0\n",
      "episode  1458  score:  200.0\n",
      "episode  1459  score:  200.0\n",
      "episode  1461  score:  200.0\n",
      "episode  1462  score:  200.0\n",
      "episode  1464  score:  200.0\n",
      "episode  1466  score:  200.0\n",
      "episode  1467  score:  200.0\n",
      "episode  1468  score:  200.0\n",
      "episode  1472  score:  200.0\n",
      "episode  1502  score:  200.0\n",
      "episode  1504  score:  200.0\n",
      "episode  1505  score:  200.0\n",
      "episode  1510  score:  200.0\n",
      "episode  1512  score:  200.0\n",
      "episode  1513  score:  200.0\n",
      "episode  1514  score:  200.0\n",
      "episode  1515  score:  197.0\n",
      "episode  1521  score:  200.0\n",
      "episode  1524  score:  200.0\n",
      "episode  1525  score:  200.0\n",
      "episode  1527  score:  200.0\n",
      "episode  1528  score:  200.0\n",
      "episode  1529  score:  200.0\n",
      "episode  1530  score:  200.0\n",
      "episode  1534  score:  200.0\n",
      "episode  1537  score:  200.0\n",
      "episode  1540  score:  200.0\n",
      "episode  1541  score:  200.0\n",
      "episode  1542  score:  200.0\n",
      "episode  1544  score:  200.0\n",
      "episode  1545  score:  200.0\n",
      "episode  1546  score:  200.0\n",
      "episode  1547  score:  200.0\n",
      "episode  1552  score:  183.0\n",
      "episode  1563  score:  200.0\n",
      "episode  1565  score:  200.0\n",
      "episode  1582  score:  200.0\n",
      "episode  1583  score:  200.0\n",
      "episode  1584  score:  200.0\n",
      "episode  1596  score:  200.0\n",
      "episode  1597  score:  200.0\n",
      "episode  1598  score:  200.0\n",
      "episode  1603  score:  200.0\n",
      "episode  1608  score:  200.0\n",
      "episode  1609  score:  200.0\n",
      "episode  1610  score:  200.0\n",
      "episode  1612  score:  200.0\n",
      "episode  1613  score:  200.0\n",
      "episode  1614  score:  200.0\n",
      "episode  1617  score:  200.0\n",
      "episode  1618  score:  200.0\n",
      "episode  1619  score:  200.0\n",
      "episode  1620  score:  200.0\n",
      "episode  1623  score:  200.0\n",
      "episode  1624  score:  200.0\n",
      "episode  1625  score:  200.0\n",
      "episode  1626  score:  200.0\n",
      "episode  1627  score:  200.0\n",
      "episode  1629  score:  200.0\n",
      "episode  1632  score:  200.0\n",
      "episode  1640  score:  200.0\n",
      "episode  1641  score:  200.0\n",
      "episode  1642  score:  200.0\n",
      "episode  1643  score:  200.0\n",
      "episode  1644  score:  200.0\n",
      "episode  1645  score:  200.0\n",
      "episode  1646  score:  200.0\n",
      "episode  1647  score:  200.0\n",
      "episode  1649  score:  200.0\n",
      "episode  1650  score:  200.0\n",
      "episode  1651  score:  200.0\n",
      "episode  1653  score:  200.0\n",
      "episode  1654  score:  200.0\n",
      "episode  1664  score:  200.0\n",
      "episode  1665  score:  200.0\n",
      "episode  1666  score:  200.0\n",
      "episode  1667  score:  200.0\n",
      "episode  1668  score:  200.0\n",
      "episode  1670  score:  200.0\n",
      "episode  1674  score:  200.0\n",
      "episode  1677  score:  200.0\n",
      "episode  1678  score:  200.0\n",
      "episode  1679  score:  200.0\n",
      "episode  1680  score:  200.0\n",
      "episode  1681  score:  200.0\n",
      "episode  1682  score:  200.0\n",
      "episode  1683  score:  200.0\n",
      "episode  1686  score:  200.0\n",
      "episode  1687  score:  200.0\n",
      "episode  1688  score:  200.0\n",
      "episode  1689  score:  200.0\n",
      "episode  1691  score:  200.0\n",
      "episode  1692  score:  200.0\n",
      "episode  1693  score:  200.0\n",
      "episode  1694  score:  200.0\n",
      "episode  1695  score:  200.0\n",
      "episode  1696  score:  200.0\n",
      "episode  1697  score:  200.0\n",
      "episode  1698  score:  200.0\n",
      "episode  1700  score:  192.0\n",
      "episode  1702  score:  200.0\n",
      "episode  1703  score:  200.0\n",
      "episode  1704  score:  200.0\n",
      "episode  1706  score:  200.0\n",
      "episode  1707  score:  200.0\n",
      "episode  1708  score:  200.0\n",
      "episode  1710  score:  200.0\n",
      "episode  1713  score:  200.0\n",
      "episode  1716  score:  200.0\n",
      "episode  1717  score:  196.0\n",
      "episode  1718  score:  200.0\n",
      "episode  1720  score:  200.0\n",
      "episode  1721  score:  200.0\n",
      "episode  1723  score:  200.0\n",
      "episode  1724  score:  200.0\n",
      "episode  1726  score:  200.0\n",
      "episode  1727  score:  200.0\n",
      "episode  1728  score:  200.0\n",
      "episode  1729  score:  200.0\n",
      "episode  1730  score:  197.0\n",
      "episode  1731  score:  200.0\n",
      "episode  1733  score:  200.0\n",
      "episode  1734  score:  200.0\n",
      "episode  1735  score:  200.0\n",
      "episode  1736  score:  200.0\n",
      "episode  1737  score:  200.0\n",
      "episode  1738  score:  200.0\n",
      "episode  1739  score:  200.0\n",
      "episode  1741  score:  200.0\n",
      "episode  1742  score:  200.0\n",
      "episode  1743  score:  200.0\n",
      "episode  1745  score:  200.0\n",
      "episode  1747  score:  200.0\n",
      "episode  1748  score:  200.0\n",
      "episode  1749  score:  200.0\n",
      "episode  1750  score:  200.0\n",
      "episode  1754  score:  187.0\n",
      "episode  1756  score:  200.0\n",
      "episode  1758  score:  200.0\n",
      "episode  1760  score:  200.0\n",
      "episode  1761  score:  200.0\n",
      "episode  1762  score:  200.0\n",
      "episode  1764  score:  200.0\n",
      "episode  1765  score:  200.0\n",
      "episode  1766  score:  200.0\n",
      "episode  1767  score:  200.0\n",
      "episode  1769  score:  200.0\n",
      "episode  1770  score:  200.0\n",
      "episode  1771  score:  200.0\n",
      "episode  1772  score:  187.0\n",
      "episode  1773  score:  200.0\n",
      "episode  1774  score:  196.0\n",
      "episode  1775  score:  200.0\n",
      "episode  1776  score:  200.0\n",
      "episode  1778  score:  200.0\n",
      "episode  1788  score:  189.0\n",
      "episode  1797  score:  196.0\n",
      "episode  1798  score:  200.0\n",
      "episode  1799  score:  200.0\n",
      "episode  1800  score:  200.0\n",
      "episode  1802  score:  200.0\n",
      "episode  1803  score:  200.0\n",
      "episode  1804  score:  200.0\n",
      "episode  1805  score:  192.0\n",
      "episode  1806  score:  200.0\n",
      "episode  1810  score:  200.0\n",
      "episode  1811  score:  200.0\n",
      "episode  1827  score:  200.0\n",
      "episode  1828  score:  185.0\n",
      "episode  1829  score:  200.0\n",
      "episode  1830  score:  200.0\n",
      "episode  1831  score:  200.0\n",
      "episode  1832  score:  200.0\n",
      "episode  1833  score:  200.0\n",
      "episode  1840  score:  200.0\n",
      "episode  1841  score:  200.0\n",
      "episode  1842  score:  200.0\n",
      "episode  1843  score:  197.0\n",
      "episode  1844  score:  200.0\n",
      "episode  1847  score:  200.0\n",
      "episode  1848  score:  200.0\n",
      "episode  1850  score:  183.0\n",
      "episode  1852  score:  190.0\n",
      "episode  1861  score:  200.0\n",
      "episode  1866  score:  195.0\n",
      "episode  1868  score:  200.0\n",
      "episode  1871  score:  194.0\n",
      "episode  1873  score:  200.0\n",
      "episode  1874  score:  200.0\n",
      "episode  1876  score:  200.0\n",
      "episode  1877  score:  200.0\n",
      "episode  1878  score:  200.0\n",
      "episode  1879  score:  200.0\n",
      "episode  1880  score:  200.0\n",
      "episode  1881  score:  200.0\n",
      "episode  1882  score:  200.0\n",
      "episode  1883  score:  200.0\n",
      "episode  1884  score:  200.0\n",
      "episode  1886  score:  200.0\n",
      "episode  1887  score:  200.0\n",
      "episode  1888  score:  200.0\n",
      "episode  1890  score:  200.0\n",
      "episode  1892  score:  200.0\n",
      "episode  1893  score:  200.0\n",
      "episode  1894  score:  200.0\n",
      "episode  1895  score:  200.0\n",
      "episode  1898  score:  200.0\n",
      "episode  1905  score:  200.0\n",
      "episode  1906  score:  200.0\n",
      "episode  1907  score:  200.0\n",
      "episode  1908  score:  184.0\n",
      "episode  1909  score:  200.0\n",
      "episode  1912  score:  200.0\n",
      "episode  1913  score:  200.0\n",
      "episode  1914  score:  200.0\n",
      "episode  1918  score:  200.0\n",
      "episode  1919  score:  200.0\n",
      "episode  1921  score:  200.0\n",
      "episode  1924  score:  200.0\n",
      "episode  1925  score:  200.0\n",
      "episode  1926  score:  200.0\n",
      "episode  1927  score:  200.0\n",
      "episode  1935  score:  200.0\n",
      "episode  1936  score:  187.0\n",
      "episode  1938  score:  200.0\n",
      "episode  1941  score:  200.0\n",
      "episode  1942  score:  200.0\n",
      "episode  1954  score:  200.0\n",
      "episode  1955  score:  200.0\n",
      "episode  1956  score:  200.0\n",
      "episode  1957  score:  200.0\n",
      "episode  1958  score:  200.0\n",
      "episode  1959  score:  200.0\n",
      "episode  1960  score:  199.0\n",
      "episode  1961  score:  200.0\n",
      "episode  1962  score:  200.0\n",
      "episode  1963  score:  200.0\n",
      "episode  1964  score:  200.0\n",
      "episode  1965  score:  200.0\n",
      "episode  1982  score:  200.0\n",
      "episode  1983  score:  200.0\n",
      "episode  1984  score:  200.0\n",
      "episode  1985  score:  200.0\n",
      "episode  1986  score:  200.0\n",
      "episode  1987  score:  200.0\n",
      "episode  1988  score:  200.0\n",
      "episode  1989  score:  200.0\n",
      "episode  1990  score:  200.0\n",
      "episode  1991  score:  200.0\n",
      "episode  1993  score:  200.0\n",
      "episode  1996  score:  200.0\n",
      "episode  1997  score:  188.0\n",
      "episode  1998  score:  200.0\n",
      "episode  1999  score:  200.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent('CartPole-v0',.95,1.0)\n",
    "# Iterate the game\n",
    "#####################################\n",
    "#uncomment if u want to load weights#\n",
    "#####################################\n",
    "# agent.model.load_weights(\"cartpole3.kek\")\n",
    "\n",
    "#####################################\n",
    "#uncomment if u want to save scores#\n",
    "#####################################\n",
    "f = open('data/CPscores.txt', 'w')\n",
    "\n",
    "for e in range(2000):\n",
    "    state = agent.env.reset()\n",
    "    state_size = agent.env.observation_space.shape[0]\n",
    "    state = np.reshape(state, [1,state_size])\n",
    "    resultReward = 0\n",
    "    while True:\n",
    "        agent.env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        # reward = reward if not done else -1\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        resultReward += reward\n",
    "        if done and resultReward <= 190:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = reward\n",
    "        if done:\n",
    "            if resultReward > 180:\n",
    "                print 'episode ', e, ' score: ', resultReward\n",
    "            print >> f, resultReward #, 'ep', e\n",
    "            break\n",
    "    agent.replay(64)\n",
    "    if e % 10 == 0:\n",
    "         agent.save(\"cartpole2.kek\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-17 13:31:50,313] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent('MountainCar-v0',1.,1.0,0.9995,0.0001)\n",
    "# Iterate the game\n",
    "f=open('mountainCarFirstModel.txt','w')\n",
    "for e in range(10000):\n",
    "    state = agent.env.reset()\n",
    "    state_size = agent.env.observation_space.shape[0]\n",
    "    state = np.reshape(state, [1,state_size])\n",
    "    resultReward = 0\n",
    "    while True:\n",
    "        agent.env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        # reward = reward if not done else -1\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        resultReward += reward\n",
    "        if done:\n",
    "            if resultReward > -200:\n",
    "                print 'ep', e , 'score', resultReward\n",
    "            print >> f, resultReward\n",
    "            break\n",
    "    agent.replay(64)\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.save(\"MountainCarFirstModelWeights.kek\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.render = True\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.memory = deque(maxlen=10000) #memory size\n",
    "        self.discount_factor = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.e_min = 0.005\n",
    "        self.e_decay = 0.00002\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        '''\n",
    "        build neural net\n",
    "        :return: model - neural net object\n",
    "        '''\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        '''\n",
    "        update weights of our target model\n",
    "        :return:\n",
    "        '''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        perform agent action in a given state\n",
    "        :param state: agent state\n",
    "        :return:\n",
    "        '''\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        add <s,a,r,s'> vector to agent memory\n",
    "        :param state: agent state\n",
    "        :param action: action to perform\n",
    "        :param reward: collected reward\n",
    "        :param next_state: next state after performing actopn\n",
    "        :param done: bool variable which determines is current state terminal or not\n",
    "        :return:\n",
    "        '''\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.e_min:\n",
    "            self.epsilon -= self.e_decay\n",
    "        # print(len(self.memory))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        '''\n",
    "        train network by sampling elements from memory\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        '''\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                          np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        '''\n",
    "        loadweights of pre-trained network\n",
    "        :param name: filename to load\n",
    "        :return:\n",
    "        '''\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        '''\n",
    "        store network weights\n",
    "        :param name: filename to save\n",
    "        :return:\n",
    "        '''\n",
    "        self.model.save_weights(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "# reduce action size for mountain car problem\n",
    "action_size = 2\n",
    "agent = Agent2(state_size, action_size)\n",
    "# agent.load(\"./save/MountainCar1.kek\")\n",
    "# uncomment to save scores to a file\n",
    "#f= open('scoresCar1.txt', 'w')\n",
    "for e in range(100000):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    # print(state)\n",
    "\n",
    "    take_action = 0\n",
    "\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 4:\n",
    "            action = agent.act(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                take_action = 0\n",
    "            elif action == 1:\n",
    "                take_action = 2\n",
    "\n",
    "        next_state, reward, done, info = env.step(take_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        #reward = reward if not done else -100\n",
    "\n",
    "        agent.remember(state, take_action, reward, next_state, done)\n",
    "        agent.replay(64)\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            agent.update_target_model()\n",
    "            # print ony successful episodes\n",
    "            if score > -200:\n",
    "                print \"episode:\", e, \"  score:\", score\n",
    "            # uncomment to save scores to a file\n",
    "            #print >> f,\"episode:\", e, \"  score:\", score\n",
    "    # save model weights every 50 iterations\n",
    "    if e % 50 == 0:\n",
    "        agent.save(\"MountainCar1.kek\")\n",
    "# file close, uncomment if you want to use file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
